(window.webpackJsonp=window.webpackJsonp||[]).push([[83],{775:function(t,s,a){"use strict";a.r(s);var i=a(1),e=Object(i.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("details",{staticClass:"custom-block details"},[a("summary",[t._v("参考资料")]),t._v(" "),a("ol",[a("li",[a("a",{attrs:{href:"http://www.xiaobaixitong.com/jiaocheng/41105.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("通俗易懂的显卡参数详解"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"http://www.skcircle.com/?id=1292",target:"_blank",rel:"noopener noreferrer"}},[t._v("深度学习显卡选型指南:关于GPU选择的一般建议"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://ai-wx.blog.csdn.net/article/details/118751181",target:"_blank",rel:"noopener noreferrer"}},[t._v("深度学习GPU显卡选型攻略"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://blog.csdn.net/weixin_29009501/article/details/112160280",target:"_blank",rel:"noopener noreferrer"}},[t._v("深度学习 训练吃显卡_深度学习显卡参数详细对比"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://blog.csdn.net/Deeachain/article/details/106690556",target:"_blank",rel:"noopener noreferrer"}},[t._v("深度学习的GPU型号和参数选择"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://wap.zol.com.cn/ask/details_2575011_3084537_3.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("显卡的主时钟频率和显存频率分别表示了显卡的哪个方面性能"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://blog.51cto.com/professor/1573126",target:"_blank",rel:"noopener noreferrer"}},[t._v("如何分辨显卡的重要参数（重要参数详解）"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://baike.baidu.com/item/%E6%98%BE%E5%AD%98%E5%AE%B9%E9%87%8F/275347",target:"_blank",rel:"noopener noreferrer"}},[t._v("显存容量 - 百度百科"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://zh.m.wikipedia.org/zh-hans/%E6%98%BE%E5%AD%98",target:"_blank",rel:"noopener noreferrer"}},[t._v("显存 - 维基百科"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://blog.csdn.net/qq_41070955/article/details/108269915",target:"_blank",rel:"noopener noreferrer"}},[t._v("NVIDIA英伟达GPU显卡算力一览（包含Tesla和GeForce、TITAN及RTX系列等）"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/394352476",target:"_blank",rel:"noopener noreferrer"}},[t._v("NVIDIA GPU 架构梳理"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://blog.csdn.net/weixin_44479045/article/details/120873332",target:"_blank",rel:"noopener noreferrer"}},[t._v("FLOPS和FLOPs、GFLOPs区别与计算"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://blog.csdn.net/zong596568821xp/article/details/103957058",target:"_blank",rel:"noopener noreferrer"}},[t._v("NVIDIA GPU的浮点计算能力"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://blog.csdn.net/sinat_24143931/article/details/78557852",target:"_blank",rel:"noopener noreferrer"}},[t._v("双精度，单精度和半精度"),a("OutboundLink")],1)])])]),t._v(" "),a("h2",{attrs:{id:"前言-关于显卡的一系列事"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#前言-关于显卡的一系列事"}},[t._v("#")]),t._v(" 前言（关于显卡的一系列事）")]),t._v(" "),a("blockquote",[a("p",[t._v("这里会简单聊聊当下显卡的背景环境以及近期的一些相关事情。正文在下一节。")])]),t._v(" "),a("p",[t._v("显卡对于炼丹人来说非常重要，因为它相当于是一个炼丹炉。能多久得到一个优良的“丹”，有一部分就要取决于炼丹炉的质地。俗话说，工欲善其事，必先利其器，挑选一个合适的显卡对于炼丹人而言意义不小。")]),t._v(" "),a("p",[t._v("但目前依旧比较尴尬的是，我们惦念的好显卡，普遍价格非常高，其实这里面就有挖矿行业和游戏行业推动的原因。我们想要用上好些的显卡来进行训练、推断等等作业，就要不得不支付一大笔费用，甚至可能这样的费用普通人是承担不起的。")]),t._v(" "),a("p",[t._v("后来我考虑了一个选择：那我退而求其次，我买一个能用的二手好显卡不可吗？于是我打探了各路消息，得出的结论是：跑，别碰！")]),t._v(" "),a("p",[t._v("这是因为近几年相当多的无良商家大批量采购了几近报废的“矿卡”，通过一些操作进行翻新之后充当二手卡卖，相当多的人都中了套：买来的显卡表面看上去是个新卡，应该非常不错，实际上内部却是满目疮痍，或许是这里元件烧坏，或许那里是缺失零件，等等。")]),t._v(" "),a("p",[t._v("而且在2021年五六月中，国家出台了政策要打压挖矿行业，可以说这真的是炼丹人的福音！然而近期知名网购平台上又出现了欺骗消费者的行为：商家将剩下的还没用完的矿卡等进行了翻新处理，再按照新卡的价格卖出。这一现象的曝光原因是有消费者收货后发现显卡非常老旧，内部的散热风扇口上也有非常重的灰尘，而店铺上写的正是新卡，当消费者与客服联系将显卡寄回店家后许久都没再受到货，联系客服也没有回音，于是这一事情在bilibili上得以曝光，有人认为邮寄回去的那个显卡被店家再翻新之后卖给了别人，真实性虽尚不可知，但这样的事本身已经足够恶劣。")]),t._v(" "),a("p",[t._v("以上种种都打消了我购置二手显卡的念头，还对全新的显卡也保有了一些戒心。因此迄今为止，我使用显卡的首要考虑途径仍然是通过关系借用工作站上的显卡，其次才是在各云GPU平台上购置会员进行使用。")]),t._v(" "),a("p",[t._v("在这里我想提一下我曾经使用过的云GPU平台Colab。我不建议在这个平台上进行相关的训练等实验，虽然，Colab在能够与Google云盘结合使用上的确非常方便，而且如果有能连接到海外的服务器，从平台到该服务器的传输速度也相当快，但是，Colab使用非常不友好。现在的Colab早就不是当年那个样子了，现在的Colab又有小会员、大会员之分，使用体验上小会员也开始和免费用户一样受到较多限制了，比如每几小时如果发现长时间不使用的话会被断线，关闭页面会导致当前的进程被服务器杀掉，而且平台上的数据也会被清空，最糟糕的是，如果不巧你需要进行高强度模型训练，Colab就会限制你的使用，我至今也没弄明白它是按照什么逻辑限制你的使用，但只要你的强度到了，它就是会提示你不可以使用，要等一段时间之后，而且也没告诉你这个“一段时间”有多久，反正不是一天两天。")]),t._v(" "),a("p",[t._v("介于这一原因，我也开始反感这类的云GPU平台，这也促使我更加想购买一个新的显卡了。之后我在相当长的时间搜集了各种资料来研究如何挑选显卡、看显卡的参数等等，遂有了这一篇文章，目的是为了给自己整顿一个采购思路。")]),t._v(" "),a("p",[t._v("至于为什么没买，肯定是因为没钱啊（悲）！")]),t._v(" "),a("h2",{attrs:{id:"概念解释"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#概念解释"}},[t._v("#")]),t._v(" 概念解释")]),t._v(" "),a("ol",[a("li",[a("strong",[t._v("显示芯片")]),t._v("： 显示芯片是显卡的核心芯片，它的性能好坏直接决定了显卡性能的好坏，它的主要任务就是处理系统输入的视频信息并将其进行构建、渲染等工作。另外，显示主芯片的性能直接决定了显示卡性能的高低。")]),t._v(" "),a("li",[a("strong",[t._v("核心频率")]),t._v("： 显示核心的核心频率在一定程度上反映出核心的运行性能，就像CPU的运行频率一样，是显卡GPU的工作频率。如果在相同核心架构的前提下，核心频率越高的显卡其运行性能就越好。核心频率的高低决定了GPU与显存之间数据交换的快慢。另外，核心频率也有其他叫法：主时钟频率。")]),t._v(" "),a("li",[a("strong",[t._v("显存速度")]),t._v("： 显存速度一般以ns（纳秒）为单位，越小表示显存的速度越快，显存的性能越好。")]),t._v(" "),a("li",[a("strong",[t._v("显存类型")]),t._v("： 显卡上采用的显存类型主要有SDR、DDR SDRAM、DDR SGRAM、DDR2、GDDR2、DDR3、GDDR3、GDDR4、GDDR5。其中，现在以GDDR3和GDDR5为主，不同的显存类型，传输效率都不一样。")]),t._v(" "),a("li",[a("strong",[t._v("显存频率")]),t._v("： 显存频率是指默认情况下，该显存在显卡上工作时的频率，以MHz（兆赫兹）为单位。显存频率一定程度上反应着该显存的速度，显存频率越高数据在显存上记录与读取的速度越快，而不同显存能提供的显存频率也差异很大。显存频率与显存时钟周期是相关的，二者成倒数关系，显存的理论工作频率计算公式是："),a("p",{staticClass:"katex-block"},[a("span",{staticClass:"katex-display"},[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"}},[a("semantics",[a("mrow",[a("mtext",[t._v("额定工作频率")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mi",[t._v("M")]),a("mi",[t._v("H")]),a("mi",[t._v("z")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mtext",[t._v("＝")]),a("mfrac",[a("mn",[t._v("1000")]),a("mtext",[t._v("显存速度")])],1),a("mo",[t._v("×")]),a("mi",[t._v("n")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\text{额定工作频率}(MHz)＝\\frac{1000}{\\text{显存速度}}\\times n\n")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"2.00744em","vertical-align":"-0.686em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("额定工作频率")])]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.10903em"}},[t._v("M")]),a("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.04398em"}},[t._v("Hz")]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mord cjk_fallback"},[t._v("＝")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mopen nulldelimiter"}),a("span",{staticClass:"mfrac"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"1.32144em"}},[a("span",{staticStyle:{top:"-2.314em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("显存速度")])])])]),a("span",{staticStyle:{top:"-3.23em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"frac-line",staticStyle:{"border-bottom-width":"0.04em"}})]),a("span",{staticStyle:{top:"-3.677em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord"},[t._v("1000")])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.686em"}},[a("span")])])])]),a("span",{staticClass:"mclose nulldelimiter"})]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("×")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.43056em","vertical-align":"0em"}}),a("span",{staticClass:"mord mathnormal"},[t._v("n")])])])])])]),t._v("\n「"),a("strong",[t._v("注")]),t._v("」：n因显存类型不同而不同，如果是SDRAM显存，则n=1；DDR显存则n=2；DDRⅡ显存则n=4")]),t._v(" "),a("li",[a("strong",[t._v("显存容量")]),t._v(": 显存容量是显卡上显存的容量数，这是选择显卡的关键参数之一。 显存容量决定着显存临时存储数据的多少，显卡显存容量有128MB、256MB、512MB、1024MB几种，64MB和128MB显存的显卡已非常少见，主流的是2GB、4GB、8GB的产品。 现如今最新显卡已经能达到1TB的显存容量（Pro SSG）。")]),t._v(" "),a("li",[a("strong",[t._v("显存位宽")]),t._v(": 显存位宽是显存在一个时钟周期内所能传送数据的位数，位数越大则瞬间所能传输的数据量越大。目前市场上的显存位宽有64位、128位、256位、384位、448位和512位等。"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mtext",[t._v("显存位宽＝显存颗粒位宽")]),a("mo",[t._v("×")]),a("mtext",[t._v("显存颗粒数")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\text{显存位宽}＝\\text{显存颗粒位宽}\\times\\text{显存颗粒数}")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.76666em","vertical-align":"-0.08333em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("显存位宽")])]),a("span",{staticClass:"mord cjk_fallback"},[t._v("＝")]),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("显存颗粒位宽")])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("×")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.68333em","vertical-align":"0em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("显存颗粒数")])])])])]),t._v("。")]),t._v(" "),a("li",[a("strong",[t._v("显卡带宽")]),t._v(": 指显示芯片与显存之间数据传输速率，它以字节/秒为单位，是决定显卡性能和速度的主要因素，计算公式为："),a("p",{staticClass:"katex-block"},[a("span",{staticClass:"katex-display"},[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"}},[a("semantics",[a("mrow",[a("mtext",[t._v("显存带宽＝")]),a("mfrac",[a("mrow",[a("mtext",[t._v("工作频率")]),a("mo",[t._v("×")]),a("mtext",[t._v("显存位宽")])],1),a("mn",[t._v("8")])],1),a("mtext",[t._v("bit")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\text{显存带宽}＝\\frac{\\text{工作频率}\\times\\text{显存位宽}}{8}\\text{bit}\n")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"2.04633em","vertical-align":"-0.686em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("显存带宽")])]),a("span",{staticClass:"mord cjk_fallback"},[t._v("＝")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mopen nulldelimiter"}),a("span",{staticClass:"mfrac"},[a("span",{staticClass:"vlist-t vlist-t2"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"1.36033em"}},[a("span",{staticStyle:{top:"-2.314em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord"},[t._v("8")])])]),a("span",{staticStyle:{top:"-3.23em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"frac-line",staticStyle:{"border-bottom-width":"0.04em"}})]),a("span",{staticStyle:{top:"-3.677em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"3em"}}),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("工作频率")])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("×")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("显存位宽")])])])])]),a("span",{staticClass:"vlist-s"},[t._v("​")])]),a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.686em"}},[a("span")])])])]),a("span",{staticClass:"mclose nulldelimiter"})]),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("bit")])])])])])])]),t._v("\n「"),a("strong",[t._v("注")]),t._v("」：显卡位宽和显卡带宽是两个不同的概念的，不过两者的关系也联系紧密，在显存频率相当的情况下，显存位宽将决定显存带宽的大小。目前大多中低端的显卡都能提供6.4GB/s至60GB/s的显存带宽，而对于中高端的显卡产品则提供超过60GB/s的显存带宽。")]),t._v(" "),a("li",[a("strong",[t._v("流处理单元")]),t._v(": 在DX10显卡出来以前，并没有“流处理器”这个说法，在DX10的时代，取消了传统的“像素管线”和“顶点管线”，统一改为流处理器单元，它既可以进行顶点运算也可以进行像素运算。")]),t._v(" "),a("li",[a("strong",[t._v("显存封装")]),t._v(": 显存封装类型基本分TSOP、QFP和BGA三类，现在基本以BGA为主。")]),t._v(" "),a("li",[a("strong",[t._v("3D API")]),t._v(": 3D API是指显卡与应用程序直接的接口。目前个人电脑中主要应用的3D API有DirectX和OpenGL。")]),t._v(" "),a("li",[a("strong",[t._v("输出接口")]),t._v(": 现在目前基本都是VGA、DVI、HDMI、DP接口。以前都是以VGA接口为主，随着技术的发展，现在反而更多的是用DVI和HDMI，一般对接口要求都不是很多。")]),t._v(" "),a("li",[a("strong",[t._v("浮点计算能力")]),t._v("： 要谈论浮点计算能力首先得区分不同精度的浮点数：半精度、单精度、双精度三种（三者都是IEEE 754标准定义的）。深度学习对精度要求往往不高，因而通常我们关注单精度浮点计算能力。在考虑尽可能节省数据传输与存储成本的情况下我们关注半精度浮点计算能力，而在一些如医学图像等对精度要求比较高的特殊领域里我们更加关注双精度浮点数，因为双精度64位，单精度32位，半精度16位。")])]),t._v(" "),a("hr"),t._v(" "),a("p",[t._v("GPU的浮点计算理论峰值能力的计算公式：")]),t._v(" "),a("p",{staticClass:"katex-block"},[a("span",{staticClass:"katex-display"},[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"}},[a("semantics",[a("mrow",[a("mtext",[t._v("理论峰值＝GPU芯片数量")]),a("mo",[t._v("×")]),a("mtext",[t._v("GPU Boost主频")]),a("mo",[t._v("×")]),a("mtext",[t._v("核心数量")]),a("mo",[t._v("×")]),a("mtext",[t._v("单个时钟周期内能处理的浮点计算次数")]),a("mspace",{attrs:{linebreak:"newline"}}),a("mtext",[t._v("双精度理论峰值＝FP64 Cores")]),a("mo",[t._v("×")]),a("mtext",[t._v("GPU Boost Clock")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mtext",[t._v("GHz")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",[t._v("×")]),a("mn",[t._v("2")]),a("mspace",{attrs:{linebreak:"newline"}}),a("mtext",[t._v("单精度理论峰值＝FP32 cores")]),a("mo",[t._v("×")]),a("mtext",[t._v("GPU Boost Clock")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mtext",[t._v("GHz")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",[t._v("×")]),a("mn",[t._v("2")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\text{理论峰值} ＝ \\text{GPU芯片数量} \\times \\text{GPU Boost主频} \\times \\text{核心数量} \\times \\text{单个时钟周期内能处理的浮点计算次数} \\\\\n\\text{双精度理论峰值} ＝ \\text{FP64 Cores} \\times \\text{GPU Boost Clock}(\\text{GHz}) \\times 2 \\\\\n\\text{单精度理论峰值} ＝ \\text{FP32 cores} \\times \\text{GPU Boost Clock}(\\text{GHz}) \\times 2\n")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.76666em","vertical-align":"-0.08333em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("理论峰值")])]),a("span",{staticClass:"mord cjk_fallback"},[t._v("＝")]),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("GPU")]),a("span",{staticClass:"mord cjk_fallback"},[t._v("芯片数量")])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("×")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.76666em","vertical-align":"-0.08333em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("GPU Boost")]),a("span",{staticClass:"mord cjk_fallback"},[t._v("主频")])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("×")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.76666em","vertical-align":"-0.08333em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("核心数量")])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("×")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.68333em","vertical-align":"0em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("单个时钟周期内能处理的浮点计算次数")])])]),a("span",{staticClass:"mspace newline"}),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.76666em","vertical-align":"-0.08333em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("双精度理论峰值")])]),a("span",{staticClass:"mord cjk_fallback"},[t._v("＝")]),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("FP64 Cores")])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("×")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("GPU Boost Clock")])]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("GHz")])]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("×")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.64444em","vertical-align":"0em"}}),a("span",{staticClass:"mord"},[t._v("2")])]),a("span",{staticClass:"mspace newline"}),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.76666em","vertical-align":"-0.08333em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("单精度理论峰值")])]),a("span",{staticClass:"mord cjk_fallback"},[t._v("＝")]),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("FP32 cores")])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("×")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("GPU Boost Clock")])]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("GHz")])]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("×")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.64444em","vertical-align":"0em"}}),a("span",{staticClass:"mord"},[t._v("2")])])])])])]),t._v(" "),a("p",[t._v("注：由于P100还支持在一个FP32里同时进行2次FP16的半精度浮点计算，所以对于半精度的理论峰值更是单精度浮点数计算能力的两倍。")]),t._v(" "),a("hr"),t._v(" "),a("p",[t._v("FLOPS是Floating-Point Operations Per Second的缩写，意指每秒浮点运算次数。用来衡量硬件的性能。而FLOPs是Floating Point of Operations的缩写，是浮点运算次数，可以用来衡量算法/模型复杂度。")]),t._v(" "),a("p",[t._v("FLOPS的单位换算")]),t._v(" "),a("p",{staticClass:"katex-block"},[a("span",{staticClass:"katex-display"},[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"}},[a("semantics",[a("mrow",[a("mn",[t._v("1")]),a("mtext"),a("mtext",[t._v("MFLOPS")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mtext",[t._v("Mega FLOPS")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",[t._v("=")]),a("mn",[t._v("1")]),a("msup",[a("mn",[t._v("0")]),a("mn",[t._v("6")])],1),a("mo",[t._v("⇒")]),a("mtext",[t._v("每秒一百万次的浮点运算")]),a("mspace",{attrs:{linebreak:"newline"}}),a("mn",[t._v("1")]),a("mtext"),a("mtext",[t._v("GFLOPS")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mtext",[t._v("Giga FLOPS")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",[t._v("=")]),a("mn",[t._v("1")]),a("msup",[a("mn",[t._v("0")]),a("mn",[t._v("9")])],1),a("mo",[t._v("⇒")]),a("mtext",[t._v("每秒十亿次的浮点运算")]),a("mspace",{attrs:{linebreak:"newline"}}),a("mn",[t._v("1")]),a("mtext"),a("mtext",[t._v("TFLOPS")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mtext",[t._v("Tera FLOPS")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",[t._v("=")]),a("mn",[t._v("1")]),a("msup",[a("mn",[t._v("0")]),a("mn",[t._v("12")])],1),a("mo",[t._v("⇒")]),a("mtext",[t._v("每秒一万亿次的浮点运算")]),a("mspace",{attrs:{linebreak:"newline"}}),a("mn",[t._v("1")]),a("mtext"),a("mtext",[t._v("PFLOPS")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mtext",[t._v("Peta FLOPS")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",[t._v("=")]),a("mn",[t._v("1")]),a("msup",[a("mn",[t._v("0")]),a("mn",[t._v("15")])],1),a("mo",[t._v("⇒")]),a("mtext",[t._v("每秒一千万亿次的浮点运算")]),a("mspace",{attrs:{linebreak:"newline"}}),a("mn",[t._v("1")]),a("mtext"),a("mtext",[t._v("EFLOPS")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mtext",[t._v("Exa FLOPS")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",[t._v("=")]),a("mn",[t._v("1")]),a("msup",[a("mn",[t._v("0")]),a("mn",[t._v("18")])],1),a("mo",[t._v("⇒")]),a("mtext",[t._v("每秒一百京次的浮点运算")]),a("mspace",{attrs:{linebreak:"newline"}}),a("mn",[t._v("1")]),a("mtext"),a("mtext",[t._v("ZFLOPS")]),a("mo",{attrs:{stretchy:"false"}},[t._v("(")]),a("mtext",[t._v("Zetta FLOPS")]),a("mo",{attrs:{stretchy:"false"}},[t._v(")")]),a("mo",[t._v("=")]),a("mn",[t._v("1")]),a("msup",[a("mn",[t._v("0")]),a("mn",[t._v("21")])],1),a("mo",[t._v("⇒")]),a("mtext",[t._v("每秒十万京次的浮点运算")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("1\\,\\text{MFLOPS}(\\text{Mega FLOPS}) = 10^{6}  \\Rightarrow \\text{每秒一百万次的浮点运算} \\\\\n1\\,\\text{GFLOPS}(\\text{Giga FLOPS}) = 10^{9}  \\Rightarrow \\text{每秒十亿次的浮点运算} \\\\\n1\\,\\text{TFLOPS}(\\text{Tera FLOPS}) = 10^{12} \\Rightarrow \\text{每秒一万亿次的浮点运算} \\\\\n1\\,\\text{PFLOPS}(\\text{Peta FLOPS}) = 10^{15} \\Rightarrow \\text{每秒一千万亿次的浮点运算} \\\\\n1\\,\\text{EFLOPS}(\\text{Exa FLOPS}) = 10^{18} \\Rightarrow \\text{每秒一百京次的浮点运算} \\\\\n1\\,\\text{ZFLOPS}(\\text{Zetta FLOPS}) = 10^{21} \\Rightarrow \\text{每秒十万京次的浮点运算}\n")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord"},[t._v("1")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("MFLOPS")])]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("Mega FLOPS")])]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.8641079999999999em","vertical-align":"0em"}}),a("span",{staticClass:"mord"},[t._v("1")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord"},[t._v("0")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8641079999999999em"}},[a("span",{staticStyle:{top:"-3.113em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[t._v("6")])])])])])])])])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("⇒")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.68333em","vertical-align":"0em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("每秒一百万次的浮点运算")])])]),a("span",{staticClass:"mspace newline"}),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord"},[t._v("1")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("GFLOPS")])]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("Giga FLOPS")])]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.8641079999999999em","vertical-align":"0em"}}),a("span",{staticClass:"mord"},[t._v("1")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord"},[t._v("0")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8641079999999999em"}},[a("span",{staticStyle:{top:"-3.113em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[t._v("9")])])])])])])])])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("⇒")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.68333em","vertical-align":"0em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("每秒十亿次的浮点运算")])])]),a("span",{staticClass:"mspace newline"}),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord"},[t._v("1")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("TFLOPS")])]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("Tera FLOPS")])]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.8641079999999999em","vertical-align":"0em"}}),a("span",{staticClass:"mord"},[t._v("1")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord"},[t._v("0")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8641079999999999em"}},[a("span",{staticStyle:{top:"-3.113em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[t._v("12")])])])])])])])])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("⇒")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.68333em","vertical-align":"0em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("每秒一万亿次的浮点运算")])])]),a("span",{staticClass:"mspace newline"}),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord"},[t._v("1")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("PFLOPS")])]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("Peta FLOPS")])]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.8641079999999999em","vertical-align":"0em"}}),a("span",{staticClass:"mord"},[t._v("1")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord"},[t._v("0")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8641079999999999em"}},[a("span",{staticStyle:{top:"-3.113em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[t._v("15")])])])])])])])])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("⇒")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.68333em","vertical-align":"0em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("每秒一千万亿次的浮点运算")])])]),a("span",{staticClass:"mspace newline"}),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord"},[t._v("1")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("EFLOPS")])]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("Exa FLOPS")])]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.8641079999999999em","vertical-align":"0em"}}),a("span",{staticClass:"mord"},[t._v("1")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord"},[t._v("0")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8641079999999999em"}},[a("span",{staticStyle:{top:"-3.113em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[t._v("18")])])])])])])])])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("⇒")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.68333em","vertical-align":"0em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("每秒一百京次的浮点运算")])])]),a("span",{staticClass:"mspace newline"}),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"mord"},[t._v("1")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("ZFLOPS")])]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord"},[t._v("Zetta FLOPS")])]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.8641079999999999em","vertical-align":"0em"}}),a("span",{staticClass:"mord"},[t._v("1")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord"},[t._v("0")]),a("span",{staticClass:"msupsub"},[a("span",{staticClass:"vlist-t"},[a("span",{staticClass:"vlist-r"},[a("span",{staticClass:"vlist",staticStyle:{height:"0.8641079999999999em"}},[a("span",{staticStyle:{top:"-3.113em","margin-right":"0.05em"}},[a("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),a("span",{staticClass:"sizing reset-size6 size3 mtight"},[a("span",{staticClass:"mord mtight"},[a("span",{staticClass:"mord mtight"},[t._v("21")])])])])])])])])]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),a("span",{staticClass:"mrel"},[t._v("⇒")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.68333em","vertical-align":"0em"}}),a("span",{staticClass:"mord text"},[a("span",{staticClass:"mord cjk_fallback"},[t._v("每秒十万京次的浮点运算")])])])])])])]),t._v(" "),a("h2",{attrs:{id:"选购参考建议"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#选购参考建议"}},[t._v("#")]),t._v(" 选购参考建议")]),t._v(" "),a("ol",[a("li",[t._v("算力至少在5.0以上。\n"),a("blockquote",[a("p",[t._v("这里的Compute Capability(算力)并不是我们常说的TFLOPS算力，它只是英伟达给自己支持CUDA的GPU设置的一个“版本号”而已，代表着这个GPU具备什么样的功能，版本号越高说明 GPU 的工具包越新，支持的功能越新。")])])]),t._v(" "),a("li",[t._v("显存尽量大，如果是CV领域中使用，建议至少8GB。")]),t._v(" "),a("li",[t._v("关注这些参数：GPU架构、显存带宽、显存位宽、GPU工作频率、CUDA核心数量和功耗。\n"),a("blockquote",[a("p",[t._v("GPU架构有Tesla 架构、Fermi架构、Kepler架构、Maxwell架构、Pascal架构、Volta架构、Turing架构和Ampere架构等。")])])]),t._v(" "),a("li",[t._v("深度学习注重的参数有两个，分别是显存带宽和浮点计算能力，显存带宽计算涉及到的显卡参数：显存位宽(位)、显存频率(Mhz)，单精度浮点据算能力涉及到的显卡参数：显卡主频(Mhz)、CUDA核心。")])]),t._v(" "),a("hr"),t._v(" "),a("p",[t._v("对于不同类型的神经网络，主要参考的指标是不太一样的。")]),t._v(" "),a("p",[a("strong",[t._v("卷积网络和Transformer")]),t._v("：Tensor核心数 > 单精度浮点性能 > 显存带宽 > 半精度浮点性能")]),t._v(" "),a("p",[a("strong",[t._v("循环神经网络")]),t._v("：显存带宽 > 半精度浮点性能 > Tensor核心数 > 单精度浮点性能")]),t._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("推荐阅读")]),t._v(" "),a("p",[t._v("推荐阅读文章开头参考资料中的"),a("a",{attrs:{href:"http://www.skcircle.com/?id=1292",target:"_blank",rel:"noopener noreferrer"}},[t._v("深度学习显卡选型指南:关于GPU选择的一般建议"),a("OutboundLink")],1),t._v("，采购的原则就是对症下药，按需采购。在这里放出该文章中我认为最值得读的一段：")]),t._v(" "),a("blockquote",[a("p",[t._v("现在，如果要判断显卡性能好不好，它的指标应该是带宽、FLOPS和Tensor Cores三合一。要理解这一点，我们可以看看矩阵乘积和卷积这两个最重要的张量操作是在哪里被加速的。")]),t._v(" "),a("p",[t._v("提到矩阵乘积，最简单有效的一个理念是它受带宽限制。如果你想用LSTM和其他需要经常进行大量矩阵乘积运算的RNN，那么内存带宽是GPU最重要的特性。类似地，卷积受计算速度约束，因此对于ResNets和其他CNN，GPU上的TFLOP是性能的最佳指标。")]),t._v(" "),a("p",[t._v("Tensor Cores的出现稍稍改变了上述平衡。它们是非常简单的专用计算单元，可以加速计算——但不是内存带宽 ——因此对CNN来说，如果GPU里有Tensor Core，它们带来的最大改善就是速度提高约30％到100％。")]),t._v(" "),a("p",[t._v("虽然Tensor Cores只能提高计算速度，但它们也可以使用16bit数进行计算。这对矩阵乘法也是一个巨大优势，因为在具有相同存储器带宽的矩阵中，16bit数的信息传输量是32bit数的两倍。数字存储器大小的削减对于在L1高速缓存中存储更多数字特别重要，实现了这一点，矩阵就越大，计算效率也更高。所以如果用了Tensor Cores，它对LSTM的提速效果约为20％至60％。")]),t._v(" "),a("p",[t._v("请注意，这种加速不是来自Tensor Cores本身，而是来自它们进行16bit数计算的能力。AMD GPU也支持16bit数计算，这意味着在进行乘积运算时，它们其实和具有Tensor Cores的NVIDIA显卡一样快。")]),t._v(" "),a("p",[t._v("Tensor Cores的一个大问题是它们需要16位浮点输入数据，这可能会引入一些软件支持问题，因为网络通常使用32位。如果没有16位输入，Tensor Cores将毫无用处。但是，我认为这些问题将很快得到解决，因为Tensor Cores太强大了，现在它们已经出现在消费级GPU中，未来，使用它的人只会越来越多。请注意，随着16bit数被引入深度学习，GPU内存其实也相当于翻倍了，因为现在我们可以在同样大小的内存中存储比之前多一倍的参数。")]),t._v(" "),a("p",[t._v("总的来说，最好的经验法则是：如果用RNN，请看带宽；如果用卷积，请看FLOPS；如果有钱，上Tensor Cores（除非你必须购买Tesla）。")])]),t._v(" "),a("p",[t._v("关于他的采购建议可以点击链接在文章开头处寻找。")])])])}),[],!1,null,null,null);s.default=e.exports}}]);