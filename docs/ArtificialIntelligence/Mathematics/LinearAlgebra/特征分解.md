---
title: 特征分解
date: 2022-01-23 01:13:32
author: Breeze Shane
top: false
toc: true
mathjax: true
categories: 
 - AritficialIntelligence
 - Mathematics
tags: 
 - Aritficial Intelligence
 - Mathematics
---

::: info 定义

线性代数中，特征分解（Eigendecomposition），又称谱分解（Spectral decomposition）是将矩阵分解为由其特征值和特征向量表示的矩阵之积的方法。需要注意只有对可对角化矩阵才可以施以特征分解。

:::

我们可以通过分解矩阵来发现矩阵表示成数组元素时不明显的函数性质。其中**特征分解**(Eigendecomposition)是使用最广的矩阵分解之一，即我们将矩阵分解成一组特征向量和特征值。

方阵$A$的**特征向量**(Eigenvector)是指与$A$相乘后相当于对该向量进行缩放的非零向量$v$:

$$

Av=\lambda v

$$

其中标量$\lambda$称为这个特征向量对应的特征值(Eigenvalue)。

类似地我们可以定义**左特征向量**(Left Eigenvector)$v^\top A=\lambda v^\top$，但通常我们更关注**右特征向量**(Right Eigenvector)。

若$v$是$A$的特征向量，那么$sv \,\, (s \in \mathbb{R}, \, s \neq 0)$也是$A$的特征向量。

此外，$sv$和$v$有相同的特征值。因此通常我们只考虑单位特征向量。

假设矩阵$A$有$n$个线性无关的特征向量$\{ v^{(1)}, \dots, v^{(n)} \}$，对应着特征值$\{ \lambda_1, \dots, \lambda_n \}$。我们将特征向量连接成一个矩阵，使得每一列是一个特征向量：$V=[v^{(1)}, \dots , v^{(n)}]$。类似地我们也可以将特征值连接成一个向量$\lambda=[\lambda_1,\dots,\lambda_n]^\top$。因此$A$的特征分解可以记作：

$$
A=Vdiag(\lambda)V^{-1}
$$

::: warning 注意

不是每一个矩阵都可以分解成特征值和特征向量。在某些情况下特征分解存在，但会涉及复数而非实数。但在本书中我们通常只需要分解一类有简单分解的矩阵。

:::

每个实对称矩阵都可以分解成实特征向量和实特征值：

$$
A=Q\Lambda Q^\top
$$

其中$Q$是$A$的特征向量组成的正交矩阵，$\Lambda$是对角矩阵。特征值$\Lambda_{i,i}$对应的特征向量是矩阵$Q$的第$i$列，记作$Q_{:,i}$。因为$Q$是正交矩阵，我们可以将$A$看作沿方向$v^{(i)}$延展$\lambda_i$倍的空间。

任意一个实对称矩阵$A$都有特征分解，但特征分解可能结果不唯一，但我们有这个性质：

::: info 性质

如果两个或多个特征向量拥有相同的特征值，那么在由这些特征向量产生的生成子空间中，任意一组正交向量都是该特征值对应的特征向量。

:::

因此我们可以等价地从这些特征向量中构成$Q$作为替代。并约定按降序排列$\Lambda$的元素，这样特征分解就可以是唯一的，当且仅当所有特征值都是唯一的。

矩阵的特征分解带给我们很多关于矩阵的有用信息：

1. 矩阵是奇异的，当且仅当含有0特征值。
2. 实对称矩阵的特征分解可以用于优化二次方程$f(x)=x^\top Ax$，其中限制$||x||_2=1$。当$x$等于$A$的某个特征向量时，$f$将返回其对应的特征值。在限制条件下，函数$f$的最大值是最大特征值，最小值是最小特征值。

::: tip

所有特征值都是正数的矩阵称为**正定**(Positive Definite)；

所有特征值都是非负数的矩阵称为**半正定**(Positive Semidefinite)；

所有特征值都是负数的矩阵称为**负定**(Negative Definite)；

所有特征值都是非正数的矩阵称为**半负定**(Negative Semidefinite)；

:::

半正定矩阵受到关注的原因是它们能够保证

$$
\forall x, x^\top Ax \geqslant 0
$$

此外，正定矩阵还保证

$$
x^\top Ax = 0 \Rightarrow x=0
$$